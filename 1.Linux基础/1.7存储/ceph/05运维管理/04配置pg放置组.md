### 放置组数配置

    # ceph osd pool create {pool-name} pg_num

> 必须选择`pg_num`的值，因为它（当前）无法自动计算。以下是一些常用值：

- 少于`5 OSDs`设置`pg_num`为`128`
- `5-10 OSDs`设置`pg_num`为`512`
- `10-50 OSDs`设置`pg_num`为`1024`
- 如果您有超过`50`个`OSD`，则需要使用[pgcalc](https://ceph.com/pgcalc/) 计算`pg_num`值

```shell
             (OSDs * 100)
Total PGs =  ------------
              pool size
```

> 结果应始终四舍五入到最接近的二次方,只有2的幂才能平衡放置组中对象的数量。其他值将导致数据在您的`OSD`中的不均匀分布。

例如，对于具有`200`个`OSD`和`3`个副本的池大小的群集，您可以按以下方式估计`PG`的数量：

```shell
(200 * 100)
----------- = 6667. 最接近该数值的2的幂等为8192
     3
```

### 放置组解析

放置组（PG）聚合池中的对象，因为在每个对象的基础上跟踪对象放置和对象元数据在计算上非常昂贵，
即，具有数百万个对象的系统无法在每个对象的基础上真实地跟踪放置。

`Ceph`客户端将计算对象应该在哪个放置组中。它通过散列对象`ID`并基于定义池中`pg`的数量和池的`ID`应用一个操作来实现这一点

放置组中的对象内容存储在一组`OSD`中。例如，在大小为2的复制池中，每个放置组将在两个`OSD`上存储对象.

如果`OSD#2`失败，另一个将被分配到放置组`#1`，并将填充`OSD#1`中所有对象的副本。如果池大小从2更改为3，则会为放置组分配一个附加的`OSD`，并接收放置组中所有对象的副本。

放置组不拥有`OSD`,它们与来自同一池甚至其他池的其他放置组共享`OSD`。如果`OSD#2`失败，放置组`#2`还必须使用`OSD#3`还原对象的副本。

当放置组的数量增加时，新的放置组将被分配`OSD`。`挤压`功能的结果也将更改，以前放置组中的某些对象将复制到新放置组中，并从旧放置组中删除。

### 放置组权衡

数据持久性和所有`OSD`之间的均匀分布需要更多的放置组，但是它们的数量应该减少到最小以节省`CPU`和内存。

> 数据持久性

单个`OSD`故障后，数据丢失的风险会增大，直到其中包含的数据被完全恢复。让我们想象一个场景，在单个放置组中导致永久的数据丢失:

- `OSD`故障（可看作磁盘故障），包含对象的所有副本丢失。对于放置组内的所有对象，副本的数量突然从3个下降到2个。
- `Ceph`通过选择一个新的`OSD`来重新创建所有对象的第三个副本，从而开始恢复此放置组。
- 同一放置组中的另一个`OSD`在新`OSD`完全填充第三个拷贝之前失败。一些对象将只有一个幸存的副本。
- `Ceph`会选择另一个`OSD`，并继续复制对象，以恢复所需的副本数量。
- 同一放置组中的第三个`OSD`在恢复完成之前失败。如果此`OSD`包含对象的唯一剩余副本，则它将永久丢失。

在一个包含10个`OSD`、在3个复制池中有512个放置组的集群中，`CRUSH`将给每个放置组三个`OSD`。
最终，每个`OSD`将托管(512 * 3)/ 10 ~= 150个放置组。
因此，当第一个`OSD`失败时，上述场景将同时启动所有150个安置组的恢复。

正在恢复的150个安置组可能均匀地分布在剩余的9个`OSDs`上。
因此，每个剩余的`OSD`都可能向其他所有`OSD`发送对象副本，并接收一些新对象来存储，因为它们成为了新的放置组的一部分。

完成此恢复所需的时间完全取决于`Ceph`集群的体系结构。假设每个`OSD`由一台机器上的1`TB` `SSD`托管，
所有这些`OSD`都连接到`10Gb/s`交换机，单个`OSD`的恢复在M分钟内完成。
如果每台机器有两个`OSD`、没有`SSD`、`1Gb/s`交换机，它至少会慢一个数量级。

在这种大小的集群中，放置组的数量对数据持f久性几乎没有影响。它可能是128或8192，恢复不会慢或快。

但是，将同一个`Ceph`集群增加到20个`OSD`而不是10个`OSD`可能会加快恢复速度，从而显著提高数据持久性。
每个`OSD`现在只参与~75个放置组，而不是在只有10个`OSD`时参与~150个，而且仍然需要剩下的19个`OSD`执行相同数量的对象副本才能恢复。
但是，以前10个`OSD`每个必须复制大约`100GB`，现在它们每个必须复制`50GB`。
如果网络是瓶颈，那么恢复的速度将是现在的两倍。换句话说，当`OSDs`的数量增加时，恢复速度会更快。

如果这个集群增长到40个`OSD`，那么每个`OSD`只能承载35个放置组。
如果`OSD`故障，除非它被另一个瓶颈阻塞，否则恢复将继续加快。
但是，如果这个集群增长到200个`OSD`，每个`OSD`只能承载~7个放置组。
如果一个`OSD`故障，那么在这些放置组中最多21个`OSD`（7*3）之间会发生恢复：恢复所需的时间将比有40个`OSD`时的集群长，这意味着放置组的数量应该增加。

无论恢复时间有多短，第二个`OSD`都有可能在恢复过程中失败。
在上述10个`osd`集群中，如果其中任何一个失败，那么~17个放置组（即~150/9个正在恢复的放置组）将只有一个幸存副本。
如果剩余的8个`OSD`中的任何一个失败，那么两个放置组的最后一个对象很可能会丢失（即大约17/8个放置组，只恢复了剩余的一个副本）。

当群集的大小增加到20个`osd`时，由于丢失3个`osd`而损坏的放置组的数量会下降。
丢失的第二个`OSD`将降级~4（即恢复~75/19个放置组），而不是~17，并且丢失的第三个`OSD`只有在包含幸存副本的四个`OSD`之一时才会丢失数据。
换句话说，如果在恢复时间范围内丢失一个`OSD`的概率为0.0001%，则在具有10个`OSD`的集群中，丢失的概率从17×10×0.0001%变为具有20个`OSD`的集群中的4×20×0.0001%

**简而言之，更多的`osd`意味着更快的恢复和更低的导致永久性丢失放置组的级联故障风险。就数据持久性而言，512或4096个放置组在少于50个`osd`的集群中大致相当。**

> 池中的对象分布

理想情况下，对象均匀分布在每个放置组中。由于`CRUSH`计算每个对象的放置组，
但实际上不知道该放置组中的每个`OSD`中存储了多少数据，因此放置组的数量与`OSD`的数量之间的比率可能会显著影响数据的分布。

例如，如果在一个三副本池中有十个`OSD`的单个放置组，那么只会使用三个`OSD`，因为`CRUSH`没有其他选择。
当有更多的放置组可用时，对象更可能均匀地分布在其中。`CRUSH`还尽一切努力将`osd`均匀地分布在所有现有的放置组中。

不均匀的数据分布可能是由osd和放置组之间的比率以外的因素造成的。
由于挤压不考虑对象的大小，一些非常大的对象可能会造成不平衡。
假设100万个4K对象（总共4GB）均匀分布在10个`OSD`上的1024个放置组中。他们将在每个`OSD`上使用`4GB/10=400MB`。
如果将一个`400MB`对象添加到池中，则支持放置该对象的放置组的三个`OSD`将被`400MB+400MB=800MB`填充，而其他七个`OSD`将仅被`400MB`占用。

> 内存、CPU和网络使用情况

对于每个放置组，`osd`和`mon`始终需要内存、网络和`CPU`，甚至在恢复期间需要更多。通过在放置组中聚集对象来共享此开销是它们存在的主要原因之一。

最小化放置组的数量可以节省大量资源。